{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-v67zBDLsBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508b4f58-9e37-4b57-cf54-3ed7c1a2a1f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/Shareddrives/'directory'/"
      ],
      "metadata": {
        "id": "jEifWXxILwGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de38ff3-3a3c-4e2e-bef2-48bed41b100f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/Prof Lu - MUNI climate change/Fall 2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre --upgrade gensim\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "gensim.__version__\n",
        "import numpy as np\n",
        "\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import pandas as pd\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import glob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S-CISXMaEMZ",
        "outputId": "b1ea80c7-5740-48a7-babb-d462ebcb6de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def article_summary(docs_df_sentence):\n",
        "  digit_pc_list = []\n",
        "  for index in range(len(docs_df_sentence)):\n",
        "    sent = docs_df_sentence.iloc[index]\n",
        "    a_id = sent['article_id']\n",
        "    for i,k in enumerate(sent['text_tokens']):\n",
        "      alphanum_cnt = 0\n",
        "      digit_chunk_cnt = 0\n",
        "      for s in k:\n",
        "        if re.match('[a-z]',s):\n",
        "          alphanum_cnt += 1\n",
        "          break\n",
        "      for s in sent['text_tokens_digit'][i]:\n",
        "        if s == '0':\n",
        "          digit_chunk_cnt +=1\n",
        "\n",
        "      if alphanum_cnt != 0 :\n",
        "        temp_dict = dict()\n",
        "        temp_dict['article_id'] = a_id\n",
        "        temp_dict['sent_id'] = i\n",
        "        temp_dict['sent_length'] = len(sent['text_tokens'][i])\n",
        "        temp_dict['digit_chunk_ratio'] = digit_chunk_cnt/len(sent['text_tokens'][i])\n",
        "        digit_pc_list.append(temp_dict)\n",
        "  digit_pc_df = pd.DataFrame(digit_pc_list)\n",
        "  return digit_pc_df\n",
        "\n",
        "\n",
        "def preprocess_adapt_sent(docs_df_sentence, sent_colname):\n",
        "\n",
        "    \"\"\"\n",
        "    Perform sentence and word tokenization for the raw text data\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    docs_df : pandas dataframe that contains documents information and raw text data\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    docs_df_sentence : pandas dataframe that contains documents information and word tokens for each sentence\n",
        "    \"\"\"\n",
        "    tqdm.pandas()\n",
        "\n",
        "    def sep_bullets(x):\n",
        "        result = []\n",
        "        for line in x:\n",
        "            result += line.split(\"â€¢\")\n",
        "        return result\n",
        "\n",
        "    # Combine digits and replace chunk of digits with 0\n",
        "    def combine_digit(x):\n",
        "        x = re.sub(\"\\d+([^A-Za-z0-9\\s]\\d+)+\",'0',x)\n",
        "        x = re.sub(\"\\d+\",'0',x)\n",
        "        return x\n",
        "\n",
        "    # Separate Bullets:\n",
        "    docs_df_sentence[\"sent_tokens\"] = docs_df_sentence[sent_colname].progress_map(sep_bullets)\n",
        "    # [End] Part 1\n",
        "\n",
        "    # convert text to lowercase\n",
        "    docs_df_sentence[\"text_tokens\"] = docs_df_sentence[\"sent_tokens\"].progress_map(lambda x: [i.lower() for i in x])\n",
        "\n",
        "    # [Added] Part 2: Cleaning Process Before Combining Chunks and Calculating Digits\n",
        "    # remove all next line\n",
        "    docs_df_sentence[\"text_tokens_helper\"] = docs_df_sentence[\"text_tokens\"].progress_map(lambda x: [re.sub(\"\\s+\", \" \", i) for i in x])\n",
        "    # [End] Part 2\n",
        "\n",
        "    # remove special characters and numbers\n",
        "    docs_df_sentence[\"text_tokens\"] = docs_df_sentence[\"text_tokens\"].progress_map(lambda x: [re.sub(\"[^a-z()]+\", \" \", i) for i in x])\n",
        "\n",
        "    # [Added] Part 3: Cleaning Process and Combining Chunks\n",
        "    # Combine chunk of digits\n",
        "    docs_df_sentence[\"text_tokens_digit\"] = docs_df_sentence[\"text_tokens_helper\"].progress_map(lambda x: [combine_digit(i) for i in x])\n",
        "    # [End] Part 3\n",
        "\n",
        "    # word tokenize every sentence\n",
        "    docs_df_sentence[\"text_tokens\"] = docs_df_sentence[\"text_tokens\"].progress_map(lambda x: [word_tokenize(i) for i in x])"
      ],
      "metadata": {
        "id": "-8uxKewGLwMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reload Saved Adapt Tokens and Raw Sentence"
      ],
      "metadata": {
        "id": "QTwpL-mgZdku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adapt_stemmed_sents = []\n",
        "topic_sent_df = []\n",
        "for i,file in enumerate(glob.glob(\"Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/A*\")):\n",
        "  print(i,file)\n",
        "  topic_sent_df_list = pd.read_csv(file)\n",
        "  topic_sent_df.append(topic_sent_df_list[['article_id','state','city_index',\n",
        "                                           'city_name', 'year', 'docs_type',\n",
        "                                           'docs_name','kept_adapt_sentence',\n",
        "                                           'text_tokens_stemmed']])\n",
        "  for k in range(len(topic_sent_df_list)):\n",
        "    temp = topic_sent_df_list.iloc[k]\n",
        "    adapt_stemmed_sents += eval(temp['text_tokens_stemmed'])\n",
        "\n",
        "topic_sent_df = pd.concat(topic_sent_df)\n",
        "topic_sent_df['kept_adapt_sentence'] = topic_sent_df['kept_adapt_sentence'].apply(lambda x:eval(x))\n",
        "topic_sent_df['text_tokens_stemmed'] = topic_sent_df['text_tokens_stemmed'].apply(lambda x:eval(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1tYn22zby7J",
        "outputId": "e3662654-a24d-4e38-a5fa-70025ccec9ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/AL_pickled_adapt_sentence.csv\n",
            "1 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/AR_pickled_adapt_sentence.csv\n",
            "2 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/AZ_pickled_adapt_sentence.csv\n",
            "3 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/CA_pickled_adapt_sentence.csv\n",
            "4 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/CO_pickled_adapt_sentence.csv\n",
            "5 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/CT_pickled_adapt_sentence.csv\n",
            "6 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/DE_pickled_adapt_sentence.csv\n",
            "7 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/DC_pickled_adapt_sentence.csv\n",
            "8 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/FL_pickled_adapt_sentence.csv\n",
            "9 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/GA_pickled_adapt_sentence.csv\n",
            "10 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/IA_pickled_adapt_sentence.csv\n",
            "11 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/ID_pickled_adapt_sentence.csv\n",
            "12 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/IL_pickled_adapt_sentence.csv\n",
            "13 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/IN_pickled_adapt_sentence.csv\n",
            "14 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/KS_pickled_adapt_sentence.csv\n",
            "15 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/KY_pickled_adapt_sentence.csv\n",
            "16 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/LA_pickled_adapt_sentence.csv\n",
            "17 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/MA_pickled_adapt_sentence.csv\n",
            "18 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/MD_pickled_adapt_sentence.csv\n",
            "19 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/ME_pickled_adapt_sentence.csv\n",
            "20 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/MI_pickled_adapt_sentence.csv\n",
            "21 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/MN_pickled_adapt_sentence.csv\n",
            "22 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/MO_pickled_adapt_sentence.csv\n",
            "23 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/MS_pickled_adapt_sentence.csv\n",
            "24 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/NC_pickled_adapt_sentence.csv\n",
            "25 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/ND_pickled_adapt_sentence.csv\n",
            "26 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/NE_pickled_adapt_sentence.csv\n",
            "27 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/NH_pickled_adapt_sentence.csv\n",
            "28 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/NJ_pickled_adapt_sentence.csv\n",
            "29 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/NM_pickled_adapt_sentence.csv\n",
            "30 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/NV_pickled_adapt_sentence.csv\n",
            "31 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/NY_pickled_adapt_sentence.csv\n",
            "32 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/OH_pickled_adapt_sentence.csv\n",
            "33 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/OK_pickled_adapt_sentence.csv\n",
            "34 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/OR_pickled_adapt_sentence.csv\n",
            "35 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/PA_pickled_adapt_sentence.csv\n",
            "36 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/RI_pickled_adapt_sentence.csv\n",
            "37 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/SC_pickled_adapt_sentence.csv\n",
            "38 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/SD_pickled_adapt_sentence.csv\n",
            "39 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/TN_pickled_adapt_sentence.csv\n",
            "40 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/TX_pickled_adapt_sentence.csv\n",
            "41 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/UT_pickled_adapt_sentence.csv\n",
            "42 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/VA_pickled_adapt_sentence.csv\n",
            "43 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/WA_pickled_adapt_sentence.csv\n",
            "44 Muni Climate Change/Processed_Pickled_adapt_sentence_230706_CSV/WI_pickled_adapt_sentence.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImuTnMPoPtDw"
      },
      "source": [
        "## Topic Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbhDmLsKUCTe"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "from gensim.models import ldamodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUhe8pxpOmkF"
      },
      "outputs": [],
      "source": [
        "def preprocess_adapt_tokens(sent):\n",
        "  stopwd = stopwords.words ('english')\n",
        "  punc = string.punctuation\n",
        "  new_sent = [[word for word in s \\\n",
        "          if word not in stopwd and word not in punc] for s in sent]\n",
        "  return new_sent\n",
        "\n",
        "def keywords_stemmer(keywords):\n",
        "\n",
        "    \"\"\"\n",
        "    Stemming the keywords, if a keyword is not a unigram, stemming each unigram separately and then combine them together\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    keywords : pandas dataframe of all the keywords\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    keywords_stem : pandas dataframe of all the keywords with column keywords_stem added\n",
        "    \"\"\"\n",
        "\n",
        "    #the stemmer requires a language parameter\n",
        "    snow_stemmer = SnowballStemmer(language = \"english\")\n",
        "\n",
        "    #stem's of each word\n",
        "    stem_words = []\n",
        "\n",
        "    for i in range(len(keywords)):\n",
        "\n",
        "        if keywords.iloc[i][\"Stemmed\"] == 1:\n",
        "\n",
        "            w = keywords.iloc[i][\"Keyword\"]\n",
        "            unigram = []\n",
        "            split_w = w.split()\n",
        "\n",
        "\n",
        "            for s in split_w:\n",
        "                x = snow_stemmer.stem(s)\n",
        "                unigram.append(x)\n",
        "\n",
        "            unigram = \" \".join(unigram)\n",
        "            stem_words.append(unigram)\n",
        "\n",
        "        else:\n",
        "\n",
        "            stem_words.append(keywords.iloc[i][\"Keyword\"])\n",
        "\n",
        "    keywords[\"keywords_stem\"] = stem_words\n",
        "    keywords = keywords.fillna(\"\")\n",
        "\n",
        "    return keywords\n",
        "\n",
        "def remove_keywords(sent, keywords):\n",
        "  new_keywords = set()\n",
        "  for k in keywords:\n",
        "    for w in k.split(\" \"):\n",
        "      new_keywords.add(w)\n",
        "  return [[word for word in s \\\n",
        "          if word not in new_keywords] for s in sent]\n",
        "\n",
        "def remove_words(sent, remove_words):\n",
        "  \"\"\"Remove single alphabet and those in remove_words \"\"\"\n",
        "  from collections import defaultdict\n",
        "  remove_words_cnt = defaultdict(int)\n",
        "  result = []\n",
        "  for s in sent:\n",
        "    temp = []\n",
        "    for word in s:\n",
        "      if len(word) ==1 or word in remove_words:\n",
        "        remove_words_cnt[word] +=1\n",
        "      else:\n",
        "        temp.append(word)\n",
        "    result.append(temp)\n",
        "  return result,remove_words_cnt\n",
        "\n",
        "def topic_model(target_details, num_topics, topicModel='lda'):\n",
        "  \"\"\"topicModel = ['lda', 'hdp']\"\"\"\n",
        "  warnings.filterwarnings(action='once')\n",
        "  dictionary = corpora.Dictionary(target_details)\n",
        "  corpus = [dictionary.doc2bow(text) for text in target_details]\n",
        "  if topicModel== 'lda':\n",
        "    model = ldamodel.LdaModel(corpus, num_topics = num_topics,\n",
        "                              id2word=dictionary, passes=15,\n",
        "                              random_state = 100)\n",
        "    topics = model.print_topics(num_words=10)\n",
        "  for topic in topics:\n",
        "      print(topic)\n",
        "  return model, topics, dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import glob\n",
        "at_least_line = 2\n",
        "keywords_adapt = pd.read_excel(\"Muni Climate Change/keywords_adaptation.xlsx\")\n",
        "keywords_adapt_stem = keywords_stemmer(keywords_adapt)"
      ],
      "metadata": {
        "id": "PxFkglkygAeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clean data and train model"
      ],
      "metadata": {
        "id": "K7IxTx2oMr-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove some common sentences\n",
        "adapt_stemmed_sents = set()\n",
        "patterns = ['Actual Actual Budget Budget'.lower(),\n",
        "            'Capital Assets Capital assets, which include'.lower(),\n",
        "            'It is responsible for the operation'.lower(),\n",
        "            \"MATA MSCAA MZS\".lower(),\n",
        "            \"Statement of Net Position\".lower(),\n",
        "            \"The following is a schedule of interfund transfers\".lower(),\n",
        "            \"The remainder of the page is intentionally left blank\".lower(),\n",
        "            'BUDGET PREPARATION WORKSHEET'.lower(),\n",
        "            'Proposed Expenditures Staffing City Department'.lower()]\n",
        "\n",
        "def any_match(x,patterns):\n",
        "  x = x.lower()\n",
        "  for p in patterns:\n",
        "    if p in x:\n",
        "      return True\n",
        "  return False\n",
        "irrelevant_adapt_sent = set()\n",
        "for k in range(len(topic_sent_df)):\n",
        "  temp = topic_sent_df.iloc[k]\n",
        "  for i,sent in enumerate(temp['kept_adapt_sentence']):\n",
        "    if any_match(sent,patterns):\n",
        "      irrelevant_adapt_sent.add(sent)\n",
        "    else:\n",
        "      adapt_stemmed_sents.add(\" \".join(temp['text_tokens_stemmed'][i]))\n",
        "adapt_stemmed_sents=[c.split(\" \") for c in adapt_stemmed_sents]"
      ],
      "metadata": {
        "id": "1GMWfd5zIjZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove some irregular words\n",
        "import time\n",
        "start_time = time.time()\n",
        "adapt_sent_processed = preprocess_adapt_tokens(adapt_stemmed_sents)\n",
        "adapt_sent_noKeywords = remove_keywords(adapt_sent_processed,\n",
        "                                        keywords_adapt_stem['keywords_stem'].tolist())\n",
        "adapt_sent_noIrrWords,irrWords_cnt = remove_words(adapt_sent_noKeywords,['project','total','fund','fy','year','citi'])\n",
        "print((time.time()-start_time)/60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mGihI0_JDPH",
        "outputId": "9bbd607a-5f2b-497c-873b-592b9761f5f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.24668237765630086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Topic Analysis\n",
        "start_time = time.time()\n",
        "tx_topic_model_dict = dict()\n",
        "model, topics, dictionary = topic_model(adapt_sent_noIrrWords,5)\n",
        "tx_topic_model_dict['lda_noIrrWords_noPatterns']={'model': model,'topics':topics,'dictionary':dictionary}\n",
        "print((time.time()-start_time)/60)"
      ],
      "metadata": {
        "id": "UDvHJp4WMQNG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d93bac1-43fb-4254-b6a6-fa852ab2add7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.032*\"park\" + 0.013*\"includ\" + 0.011*\"sidewalk\" + 0.011*\"new\" + 0.011*\"phase\" + 0.011*\"st\" + 0.010*\"avenu\" + 0.010*\"th\" + 0.008*\"creek\" + 0.008*\"upgrad\"')\n",
            "(1, '0.014*\"includ\" + 0.013*\"provid\" + 0.010*\"sewer\" + 0.010*\"maintain\" + 0.008*\"area\" + 0.008*\"complet\" + 0.008*\"develop\" + 0.008*\"increas\" + 0.007*\"public\" + 0.007*\"work\"')\n",
            "(2, '0.032*\"bond\" + 0.023*\"revenu\" + 0.020*\"capit\" + 0.019*\"tax\" + 0.018*\"general\" + 0.017*\"debt\" + 0.013*\"million\" + 0.010*\"sewer\" + 0.009*\"use\" + 0.009*\"oblig\"')\n",
            "(3, '0.029*\"public\" + 0.017*\"depart\" + 0.016*\"develop\" + 0.016*\"park\" + 0.016*\"work\" + 0.014*\"engin\" + 0.011*\"util\" + 0.011*\"fire\" + 0.011*\"communiti\" + 0.010*\"budget\"')\n",
            "(4, '0.060*\"budget\" + 0.036*\"capit\" + 0.025*\"district\" + 0.022*\"cost\" + 0.021*\"descript\" + 0.019*\"adopt\" + 0.017*\"sourc\" + 0.013*\"expenditur\" + 0.013*\"cip\" + 0.012*\"impact\"')\n",
            "3.458766504128774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sff3iputRrls"
      },
      "source": [
        "### Save Topic Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZeKYcfVRt5E"
      },
      "outputs": [],
      "source": [
        "def topic_extract(df, model_name, topic_model_dict):\n",
        "  if len(df['text_tokens_stemmed'])==0:\n",
        "    return None\n",
        "  text_tokens_stemmed_comb = [c for sent in df['text_tokens_stemmed'] for c in sent]\n",
        "  model_dict = topic_model_dict[model_name]\n",
        "  temp =model_dict['dictionary'].doc2bow(text_tokens_stemmed_comb)\n",
        "  result = []\n",
        "  for k,v in sorted(model_dict['model'][temp], key = lambda x: -x[1]):\n",
        "    result.append((k,round(v,2)))\n",
        "  return result\n",
        "\n",
        "def find_topic_pc(x,i):\n",
        "  if type(x) != list:\n",
        "    return None\n",
        "  for k,v in x:\n",
        "    if k == i:\n",
        "      return v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O42etoARRz07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "322694d4-ff95-4f91-fdaa-1f36e87f8537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        }
      ],
      "source": [
        "topic_sent_df['topic'] = topic_sent_df.apply(topic_extract,\n",
        "                                                   args = ('lda_noIrrWords_noPatterns',tx_topic_model_dict,),\n",
        "                                                   axis = 1)\n",
        "for i in range(5):\n",
        "  topic_sent_df[f'topic_{i}'] = topic_sent_df['topic'].apply(find_topic_pc, args = (i,))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topics_df = []\n",
        "for name, item in tx_topic_model_dict.items():\n",
        "  for topic_i, topic in item['topics']:\n",
        "    temp = dict()\n",
        "    temp['model'] = name\n",
        "    temp['model_path'] = f'Topic Modeling/topic_models/{name}_topic_modeling_v1.gensim'\n",
        "    item['model'].save(f'Topic Modeling/topic_models/{name}_topic_modeling_v1.gensim')\n",
        "    item['dictionary'].save(f'Topic Modeling/topic_models/{name}_topic_modeling_v1.dictionary')\n",
        "\n",
        "    temp['topic_index'] = topic_i\n",
        "    temp['topic'] = topic\n",
        "\n",
        "    topics_df.append(temp)\n",
        "topics_df = pd.DataFrame(topics_df)\n",
        "topics_df.to_csv(\"Topic Modeling/model_output(topic_definition).csv\",index = False)"
      ],
      "metadata": {
        "id": "U9_cD_59hfsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c50b8d-b071-44d2-ad03-51045944a5fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reload Topic Modeling Models"
      ],
      "metadata": {
        "id": "WDbAt12JIswr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def topic_extract(df, model, dictionary):\n",
        "  if len(df['text_tokens_stemmed'])==0:\n",
        "    return None\n",
        "  text_tokens_stemmed_comb = [c for sent in df['text_tokens_stemmed'] for c in sent]\n",
        "  temp =dictionary.doc2bow(text_tokens_stemmed_comb)\n",
        "  result = []\n",
        "  for k,v in sorted(model[temp], key = lambda x: -x[1]):\n",
        "    result.append((k,round(v,2)))\n",
        "  return result\n",
        "\n",
        "def find_topic_pc(x,i):\n",
        "  if type(x) != list:\n",
        "    return None\n",
        "  for k,v in x:\n",
        "    if k == i:\n",
        "      return v"
      ],
      "metadata": {
        "id": "bv3HOMdBpuVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'lda_noIrrWords_noPatterns'\n",
        "model = ldamodel.LdaModel.load(f'Topic Modeling/topic_models/{name}_topic_modeling_v1.gensim')\n",
        "dictionary = corpora.Dictionary.load(f'Topic Modeling/topic_models/{name}_topic_modeling_v1.dictionary')"
      ],
      "metadata": {
        "id": "WjIZx3qKIzxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_sent_df['topic'] = topic_sent_df.apply(topic_extract,\n",
        "                                                   args = (model,dictionary),\n",
        "                                                   axis = 1)\n",
        "for i in range(5):\n",
        "  topic_sent_df[f'topic_{i}'] = topic_sent_df['topic'].apply(find_topic_pc, args = (i,))"
      ],
      "metadata": {
        "id": "1mqRu_OOQ5OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_sent_ind_df = []\n",
        "for i in range(len(topic_sent_df)):\n",
        "  temp = topic_sent_df.iloc[i]\n",
        "  for raw_sent,stemmed_sent in zip(temp['kept_adapt_sentence'],temp['text_tokens_stemmed']):\n",
        "    temp_dict = dict()\n",
        "\n",
        "    for k in topic_sent_df.columns:\n",
        "      if k not in ['kept_adapt_sentence','text_tokens_stemmed']:\n",
        "        temp_dict[k] = temp[k]\n",
        "    temp_dict['kept_adapt_sentence'] = raw_sent\n",
        "    temp_dict['text_tokens_stemmed'] = stemmed_sent\n",
        "    temp_dict['topic'] = model[dictionary.doc2bow(stemmed_sent)]\n",
        "    topic_sent_ind_df.append(temp_dict)\n",
        "topic_sent_ind_df = pd.DataFrame(topic_sent_ind_df)"
      ],
      "metadata": {
        "id": "36BFIqg0SpFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_sent_ind_df = pd.DataFrame(topic_sent_ind_df)"
      ],
      "metadata": {
        "id": "qA2RJBTBYo_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  topic_sent_ind_df[f'topic_{i}'] = topic_sent_ind_df['topic'].apply(find_topic_pc, args = (i,))"
      ],
      "metadata": {
        "id": "W8Q_u1BxYvvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_sent_ind_df.to_csv(\"Topic Modeling/final_output_topics.csv\", index = False)"
      ],
      "metadata": {
        "id": "ii5uU1r_ZJAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ozdNwjZd_wW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}